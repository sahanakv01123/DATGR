{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJCgGxkkVwH7"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 1) Environment setup (Colab)\n",
        "#    - Installs pinned versions for reproducibility\n",
        "#    - Uninstall line is optional; kept commented as in your notebook\n",
        "# =========================================================\n",
        "\n",
        "# (Optional) Clean install if your Colab runtime has conflicts\n",
        "#!pip uninstall -y numpy pandas scikit-learn matplotlib seaborn networkx node2vec datasets sentence-transformers tqdm\n",
        "\n",
        "# Install pinned versions (matches the notebook run that produced your figures)\n",
        "!pip install --no-cache-dir numpy==1.26.4 pandas==2.2.2 scikit-learn==1.5.2 matplotlib==3.9.2 seaborn==0.13.2 \\\n",
        "            networkx==3.2.1 node2vec==0.5.0 datasets==3.0.1 sentence-transformers==3.0.1 tqdm==4.66.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Imports\n",
        "#  - numpy/pandas: data handling\n",
        "#  - networkx: graph construction + graph analytics\n",
        "#  - sklearn: evaluation metrics + classifier\n",
        "#  - datasets: load BIOMRC from Hugging Face\n",
        "#  - sentence-transformers: sentence embedding model (MiniLM)\n",
        "#  - node2vec: node embeddings used for link prediction features\n",
        "# =========================================================\n",
        "\n",
        "\n",
        "import numpy as np, pandas as pd, networkx as nx, matplotlib.pyplot as plt, seaborn as sns, random, math\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from node2vec import Node2Vec\n",
        "\n",
        "# Plot styling\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "print(\"Environment ready\")\n"
      ],
      "metadata": {
        "id": "mzORB6p9VzE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2) Load Dataset (BIOMRC) and build pseudo-temporal windows\n",
        "#    - BIOMRC is loaded from Hugging Face\n",
        "#    - We sample a subset (e.g., 2000 abstracts)\n",
        "#    - We split into T windows (pseudo-chronological)\n",
        "# =========================================================\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load only the default configuration\n",
        "ds = load_dataset(\"nlpaueb/BIOMRC\", \"default\")\n",
        "\n",
        "# Sample subset\n",
        "# Note: this uses shuffle(seed=42) so the subset is repeatable in Colab.\n",
        "texts = [\n",
        "    x[\"abstract\"] for x in ds[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "    if isinstance(x.get(\"abstract\"), str) and len(x[\"abstract\"]) > 0\n",
        "]\n",
        "\n",
        "# Number of time windows (T)\n",
        "T = 5\n",
        "\n",
        "# Split into windows\n",
        "windows = np.array_split(texts, T)\n",
        "\n",
        "print(f\"Loaded {len(texts)} biomedical abstracts → divided into {T} temporal windows\")\n",
        "print(\"Example abstract:\\n\", texts[0][:300], \"...\")"
      ],
      "metadata": {
        "id": "qzBt48mYV2Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3) Sentence embeddings + per-window vocabulary\n",
        "#    - Encode each window's abstracts using MiniLM\n",
        "#    - Build per-window vocab of top 400 tokens (simple split)\n",
        "# =========================================================\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Sentence encoder (normalized embeddings)\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "window_embeddings, vocab_per_window = [], []\n",
        "\n",
        "for w in tqdm(windows, desc=\"Encoding windows\"):\n",
        "    # Encode abstracts (used as a drift basis in concept, although drift below uses random vectors)\n",
        "    emb = model.encode(list(w), show_progress_bar=False, convert_to_numpy=True,\n",
        "                       normalize_embeddings=True)\n",
        "    window_embeddings.append(emb)\n",
        "\n",
        "    # Build a simple vocabulary: top 400 tokens by frequency (whitespace tokenization)\n",
        "    joined = \" \".join(w).lower().split()\n",
        "    vocab = list(pd.Series(joined).value_counts().head(400).index)\n",
        "    vocab_per_window.append(vocab)\n",
        "\n",
        "print(f\"Computed embeddings for {T} windows, vocab sizes: {[len(v) for v in vocab_per_window]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YSBQpqoaV5fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4) Graph construction and drift calculation\n",
        "#    - build_graph: co-occurrence graph within a sliding window\n",
        "#    - drift: drift is computed from RANDOM per-term vectors\n",
        "# =========================================================\n",
        "def build_graph(texts, vocab, window_size=6):  # shorter window\n",
        "    \"\"\"\n",
        "    Build a co-occurrence graph for one window.\n",
        "    - Nodes: vocab terms\n",
        "    - Edges: co-occurrence within `window_size` tokens\n",
        "    - Edge weight: co-occurrence count\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add all vocab terms as nodes\n",
        "    for v in vocab: G.add_node(v)\n",
        "\n",
        "    # Add edges based on local co-occurrence\n",
        "    for txt in texts:\n",
        "        tokens = [t for t in txt.lower().split() if t in vocab]\n",
        "        for i in range(len(tokens)-1):\n",
        "            # j is bounded by local context window\n",
        "            for j in range(i+1, min(i+window_size, len(tokens))):\n",
        "                a,b = tokens[i],tokens[j]\n",
        "                if a==b: continue\n",
        "\n",
        "                # Increment edge weight (co-occurrence frequency)\n",
        "                G.add_edge(a,b, weight=G.get_edge_data(a,b,{\"weight\":0})[\"weight\"]+1)\n",
        "    return G\n",
        "\n",
        "# Build graphs for each window\n",
        "graphs = [build_graph(list(w), vocab_per_window[t]) for t,w in enumerate(windows)]\n",
        "word_embeds = {t:{w:np.random.randn(32) for w in vocab_per_window[t]} for t in range(T)}  # 32-dim\n",
        "\n",
        "def cosine(u,v):\n",
        "    \"\"\"Cosine similarity with epsilon for numeric stability.\"\"\"\n",
        "    return np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(v)+1e-9)\n",
        "def drift_values(t):\n",
        "    \"\"\"\n",
        "    Drift between window t-1 and t:\n",
        "    D(w) = 1 - cosine(prev_vec, cur_vec)\n",
        "    \"\"\"\n",
        "    if t==0: return {}\n",
        "    prev,cur = word_embeds[t-1], word_embeds[t]\n",
        "    shared=set(prev)&set(cur)\n",
        "    return {w:1-cosine(prev[w],cur[w]) for w in shared}\n",
        "\n",
        "# Compute drift dict for each window\n",
        "drifts=[drift_values(t) for t in range(T)]"
      ],
      "metadata": {
        "id": "DMGifXpiV8Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 5) Drift-Aware Rewiring\n",
        "#    - For each edge, compute a logistic \"innovation\" term\n",
        "#    - Combine with previous weight using eta\n",
        "#    - Keep top-k neighbors per node (graph sparsification)\n",
        "# =========================================================\n",
        "\n",
        "def rewire_graph(Gt, drift_t, eta=0.2, betas=(0,3,2,1), topk=5):\n",
        "    \"\"\"\n",
        "    Rewire a graph based on drift signals.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Gt : nx.Graph\n",
        "        Graph for time window t\n",
        "    drift_t : dict\n",
        "        Drift values for nodes in window t\n",
        "    eta : float\n",
        "        Adaptation rate: weight = (1-eta)*old + eta*new\n",
        "    betas : tuple\n",
        "        (b0,b1,b2,b3) coefficients in logistic score\n",
        "    topk : int\n",
        "        Keep only top-k neighbors per node after rewiring\n",
        "    \"\"\"\n",
        "    b0, b1, b2, b3 = betas\n",
        "\n",
        "    Gnew = nx.Graph()\n",
        "    Gnew.add_nodes_from(Gt.nodes())\n",
        "\n",
        "    # Normalize edge weights by max weight for stable scoring\n",
        "    max_w = max(nx.get_edge_attributes(Gt, \"weight\").values())\n",
        "\n",
        "    for (i, j, data) in Gt.edges(data=True):\n",
        "        # Normalized co-occurrence strength\n",
        "        s_t = data[\"weight\"] / max(1, max_w)\n",
        "\n",
        "        # Small noise term\n",
        "        delta = np.random.uniform(-0.05, 0.05)\n",
        "\n",
        "        # Drift values for the endpoints (default 0 if missing)\n",
        "        Di, Dj = drift_t.get(i, 0), drift_t.get(j, 0)\n",
        "\n",
        "        # Logistic scoring function\n",
        "        score = b0 + b1*s_t + b2*delta + b3*(Di + Dj)\n",
        "\n",
        "        # Previous edge weight\n",
        "        w_prev = data[\"weight\"]\n",
        "\n",
        "        # Updated edge weight using convex combination\n",
        "        w_hat = (1-eta)*w_prev + eta*(1/(1+math.exp(-score)))\n",
        "\n",
        "        Gnew.add_edge(i, j, weight=w_hat)\n",
        "\n",
        "    # Keep only top-k neighbors per node (sparsification)\n",
        "    for n in list(Gnew.nodes()):\n",
        "        nbrs = [(v, Gnew[n][v][\"weight\"]) for v in Gnew.neighbors(n)]\n",
        "        nbrs.sort(key=lambda x: -x[1])\n",
        "\n",
        "        # Remove edges beyond top-k\n",
        "        for v, _ in nbrs[topk:]:\n",
        "            Gnew.remove_edge(n, v)\n",
        "\n",
        "    return Gnew\n",
        "\n"
      ],
      "metadata": {
        "id": "fMYEZ6y0V-qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 6) Link Prediction (Graph-Level Evaluation)\n",
        "#    Goal:\n",
        "#      Train node embeddings on Gtrain (Node2Vec),\n",
        "#      build edge-level features,\n",
        "#      and predict whether an edge appears in Gnext.\n",
        "# =========================================================\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def evaluate_link_prediction(Gtrain, Gnext, n_hard=2):\n",
        "    \"\"\"\n",
        "    Link-prediction evaluator.\n",
        "    - Positives: edges that appear in Gnext but NOT in Gtrain (new edges)\n",
        "    - Negatives: sampled non-edges, biased toward \"hard negatives\"\n",
        "      (pairs that share common neighbors or random near-misses)\n",
        "    - Model: Node2Vec + Logistic Regression (calibrated)\n",
        "    \"\"\"\n",
        "\n",
        "    # -------------------------------\n",
        "    # 1) Build positive edges\n",
        "    # -------------------------------\n",
        "    pos = [(u, v) for (u, v) in Gnext.edges() if not Gtrain.has_edge(u, v)]\n",
        "\n",
        "    # Candidate node set\n",
        "    nodes = list(Gtrain.nodes())\n",
        "\n",
        "    # -------------------------------\n",
        "    # 2) Build negative edges (hard negatives)\n",
        "    # -------------------------------\n",
        "    neg = set()\n",
        "\n",
        "    # Sample negatives until we have the same count as positives\n",
        "    while len(neg) < len(pos):\n",
        "        a, b = random.sample(nodes, 2)\n",
        "\n",
        "        # Skip if edge exists in either graph\n",
        "        if Gtrain.has_edge(a, b) or Gnext.has_edge(a, b):\n",
        "            continue\n",
        "\n",
        "        # Hardness = number of common neighbors in Gtrain\n",
        "        cn = len(list(nx.common_neighbors(Gtrain, a, b)))\n",
        "\n",
        "        # Keep pair if \"hard enough\" OR randomly keep as near-miss\n",
        "        if cn >= n_hard or random.random() < 0.2:\n",
        "            neg.add(tuple(sorted((a, b))))\n",
        "\n",
        "    neg = list(neg)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 3) Train Node2Vec embeddings on Gtrain\n",
        "    # -------------------------------\n",
        "    n2v = Node2Vec(\n",
        "        Gtrain,\n",
        "        dimensions=64,\n",
        "        walk_length=8,\n",
        "        num_walks=20,\n",
        "        quiet=True,\n",
        "        workers=2\n",
        "    )\n",
        "    model = n2v.fit(window=5, min_count=1, batch_words=256)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 4) Edge feature constructor\n",
        "    # -------------------------------\n",
        "    def edge_features(u, v):\n",
        "        \"\"\"\n",
        "        Build an edge feature vector from:\n",
        "          - embedding ops: Hadamard, L1, L2\n",
        "          - graph features: common neighbors, Jaccard approx, preferential attachment\n",
        "        \"\"\"\n",
        "        if u not in model.wv or v not in model.wv:\n",
        "            return None\n",
        "\n",
        "        zu, zv = model.wv[u], model.wv[v]\n",
        "\n",
        "        # Embedding-based composition\n",
        "        had = zu * zv\n",
        "        l1  = np.abs(zu - zv)\n",
        "        l2  = (zu - zv) ** 2\n",
        "\n",
        "        # Simple graph-based signals\n",
        "        cn = len(list(nx.common_neighbors(Gtrain, u, v)))\n",
        "\n",
        "        # Jaccard-like score (union of neighbor sets)\n",
        "        ja = sum(1 for _ in nx.common_neighbors(Gtrain, u, v)) / (\n",
        "            len(set(Gtrain[u]) | set(Gtrain[v])) + 1e-9\n",
        "        )\n",
        "\n",
        "        # Preferential attachment (degree(u)*degree(v))\n",
        "        pa = Gtrain.degree(u) * Gtrain.degree(v)\n",
        "\n",
        "        return np.concatenate([had, l1, l2, [cn, ja, pa]])\n",
        "\n",
        "    # -------------------------------\n",
        "    # 5) Build the dataset for the classifier\n",
        "    # -------------------------------\n",
        "    edges = pos + neg\n",
        "\n",
        "    # Build feature matrix\n",
        "    X = np.vstack([\n",
        "        edge_features(u, v)\n",
        "        for (u, v) in edges\n",
        "        if edge_features(u, v) is not None\n",
        "    ])\n",
        "\n",
        "    # Labels: 1 for positives, 0 for negatives\n",
        "    y = np.array([1] * len(pos) + [0] * len(neg))[: len(X)]\n",
        "\n",
        "    # -------------------------------\n",
        "    # 6) Classifier + probability calibration\n",
        "    # -------------------------------\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "\n",
        "    base = LogisticRegression(\n",
        "        max_iter=2000,\n",
        "        class_weight={0: 1.0, 1: 2.0},\n",
        "        C=0.7\n",
        "    )\n",
        "\n",
        "    # Isotonic calibration improves probability quality\n",
        "    clf = CalibratedClassifierCV(base, method=\"isotonic\", cv=3)\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    scores = clf.predict_proba(X)[:, 1]\n",
        "\n",
        "    # -------------------------------\n",
        "    # 7) Metrics\n",
        "    # -------------------------------\n",
        "    return {\n",
        "        \"auroc\": roc_auc_score(y, scores),\n",
        "        \"auprc\": average_precision_score(y, scores)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "hux6h5I_WBt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Run evaluation across windows\n",
        "# - Static: use the original graph for window t\n",
        "# - Drift-aware: rewire the graph for window t\n",
        "# - Predict edges in window t+1\n",
        "# =========================================================\n",
        "\n",
        "results = []\n",
        "\n",
        "for t in range(T-2):\n",
        "    # Training graph for current window\n",
        "    G_static = graphs[t]\n",
        "\n",
        "    # Drift-aware rewired graph (DATGR)\n",
        "    G_drift  = rewire_graph(graphs[t], drifts[t])\n",
        "\n",
        "    # Next window graph (target edges)\n",
        "    G_next   = graphs[t+1]\n",
        "\n",
        "    # Evaluate static baseline\n",
        "    r_static = evaluate_link_prediction(G_static, G_next)\n",
        "\n",
        "    # Evaluate drift-aware approach\n",
        "    r_drift  = evaluate_link_prediction(G_drift,  G_next)\n",
        "\n",
        "    # Store window-level metrics\n",
        "    results.append({\n",
        "        \"window\": t,\n",
        "        \"static_AUROC\": r_static[\"auroc\"],\n",
        "        \"drift_AUROC\":  r_drift[\"auroc\"],\n",
        "        \"static_AUPRC\": r_static[\"auprc\"],\n",
        "        \"drift_AUPRC\":  r_drift[\"auprc\"]\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "display(df)\n"
      ],
      "metadata": {
        "id": "AE-JbO0-WEt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 7) Visualization\n",
        "# - Produces paper-ready figures at 300 DPI\n",
        "# - Saves: fig_auroc.png, fig_auprc.png, fig_edits.png\n",
        "# =========================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
        "\n",
        "# --- AUROC Plot ---\n",
        "plt.figure(figsize=(7,4))\n",
        "sns.lineplot(data=df, x=\"window\", y=\"static_AUROC\", label=\"Static\", linewidth=2.5)\n",
        "sns.lineplot(data=df, x=\"window\", y=\"drift_AUROC\", label=\"Drift-Aware\", linewidth=2.5)\n",
        "plt.title(\"AUROC Across Time Windows\")\n",
        "plt.xlabel(\"Window t → t+1\"); plt.ylabel(\"AUROC\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_auroc.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# --- AUPRC Plot ---\n",
        "plt.figure(figsize=(7,4))\n",
        "sns.lineplot(data=df, x=\"window\", y=\"static_AUPRC\", label=\"Static\", linewidth=2.5)\n",
        "sns.lineplot(data=df, x=\"window\", y=\"drift_AUPRC\", label=\"Drift-Aware\", linewidth=2.5)\n",
        "plt.title(\"AUPRC Across Time Windows\")\n",
        "plt.xlabel(\"Window t → t+1\"); plt.ylabel(\"AUPRC\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_auprc.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# --- Edge Edits (Graph Drift Volume) ---\n",
        "# NOTE: This uses nx.difference(G1, G2) as in your original notebook.\n",
        "#       It is an approximate \"edit volume\" indicator.\n",
        "import networkx as nx\n",
        "\n",
        "edit_sizes = [\n",
        "    len(nx.difference(graphs[t], rewire_graph(graphs[t], drifts[t])).edges())\n",
        "    for t in range(len(graphs)-1)\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "sns.barplot(x=list(range(len(edit_sizes))), y=edit_sizes, palette=\"crest\")\n",
        "plt.title(\"Edge Edits Per Window After Drift-Aware Rewiring\")\n",
        "plt.xlabel(\"Window Index\"); plt.ylabel(\"# Edges Changed\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_edits.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: fig_auroc.png, fig_auprc.png, fig_edits.png\")\n"
      ],
      "metadata": {
        "id": "UP8um3iOWG8P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}